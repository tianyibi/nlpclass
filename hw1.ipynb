{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will start by downloading 20-newsgroup text dataset:\n",
    "\n",
    "```http://scikit-learn.org/stable/datasets/index.html#the-20-newsgroups-text-dataset```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import training and testing data from file\n",
    "import pickle as pkl\n",
    "\n",
    "\n",
    "def import_data(dir):\n",
    "    dataset = []\n",
    "    for file_dir in os.listdir(dir):\n",
    "        with open(dir+'/'+file_dir) as f:\n",
    "            dataset.append(f.read())\n",
    "    return dataset\n",
    "\n",
    "pos_train_data = import_data(\"aclImdb/train/pos\")\n",
    "neg_train_data = import_data(\"aclImdb/train/neg\")\n",
    "pos_test_data = import_data(\"aclImdb/test/pos\")\n",
    "neg_test_data = import_data(\"aclImdb/test/neg\")\n",
    "\n",
    "# concatenate positive and negative training data, testing data\n",
    "train_data = pos_train_data+neg_test_data\n",
    "train_target = [1]*len(pos_train_data)+[0]*len(neg_train_data)\n",
    "test_data = pos_test_data+neg_test_data\n",
    "test_target = [1]*len(pos_test_data)+[0]*len(neg_test_data)\n",
    "\n",
    "# split train into train and validation \n",
    "val_idx = random.sample(range(len(train_data)), 5000)\n",
    "pkl.dump(val_idx, open(\"val_idx.p\", \"wb\"))\n",
    "\n",
    "val_data = [train_data[i] for i in val_idx]\n",
    "val_target = [train_target[i] for i in val_idx]\n",
    "\n",
    "train_sub_data = [train_data[i] for i in range(len(train_data)) if i not in val_idx]\n",
    "train_sub_target = [train_target[i] for i in range(len(train_data)) if i not in val_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install spacy\n",
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's write the tokenization function \n",
    "\n",
    "import spacy\n",
    "import string\n",
    "\n",
    "# Load English tokenizer, tagger, parser, NER and word vectors\n",
    "tokenizer = spacy.load('en_core_web_sm')\n",
    "punctuations = string.punctuation\n",
    "\n",
    "# lowercase and remove punctuation\n",
    "def tokenize(sent):\n",
    "    tokens = tokenizer(sent)\n",
    "    return [token.text.lower() for token in tokens if (token.text not in punctuations)]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing val data\n",
      "Tokenizing test data\n",
      "Tokenizing train data\n"
     ]
    }
   ],
   "source": [
    "# This is the code cell that tokenizes train/val/test datasets\n",
    "# However it takes about 15-20 minutes to run it\n",
    "# For convinience we have provided the preprocessed datasets\n",
    "# Please see the next code cell\n",
    "\n",
    "def tokenize_dataset(dataset):\n",
    "    token_dataset = []\n",
    "    # we are keeping track of all tokens in dataset \n",
    "    # in order to create vocabulary later\n",
    "    all_tokens = []\n",
    "    token_dataset_raw = []\n",
    "    all_tokens_raw = []\n",
    "    \n",
    "    for sample in dataset:\n",
    "        tokens_raw = tokenizer(sample)\n",
    "        tokens = [token.text.lower() for token in tokens_raw if (token.text not in punctuations)]\n",
    "        \n",
    "        token_dataset.append(tokens)\n",
    "        token_dataset_raw.append(tokens_raw)\n",
    "        \n",
    "        all_tokens += tokens\n",
    "        all_tokens_raw += tokens_raw\n",
    "\n",
    "    return token_dataset, all_tokens, token_dataset_raw, all_tokens_raw\n",
    "\n",
    "# val set tokens\n",
    "print (\"Tokenizing val data\")\n",
    "val_data_tokens, _ = tokenize_dataset(val_data)\n",
    "pkl.dump(val_data_tokens, open(\"val_data_tokens.p\", \"wb\"))\n",
    "pkl.dump(val_target, open(\"val_target.p\", \"wb\"))\n",
    "\n",
    "# test set tokens\n",
    "print (\"Tokenizing test data\")\n",
    "test_data_tokens, _ = tokenize_dataset(test_data)\n",
    "pkl.dump(test_data_tokens, open(\"test_data_tokens.p\", \"wb\"))\n",
    "pkl.dump(test_target, open(\"test_target.p\", \"wb\"))\n",
    "\n",
    "# train set tokens\n",
    "print (\"Tokenizing train data\")\n",
    "train_data_tokens, all_train_tokens = tokenize_dataset(train_sub_data)\n",
    "pkl.dump(train_data_tokens, open(\"train_data_tokens.p\", \"wb\"))\n",
    "pkl.dump(all_train_tokens, open(\"all_train_tokens.p\", \"wb\"))\n",
    "pkl.dump(train_sub_target, open(\"train_target.p\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing val data\n",
      "Tokenizing test data\n",
      "Tokenizing train data\n"
     ]
    }
   ],
   "source": [
    "# without post-processing\n",
    "def tokenize_dataset_raw(dataset):\n",
    "    token_dataset = []\n",
    "    # we are keeping track of all tokens in dataset \n",
    "    # in order to create vocabulary later\n",
    "    all_tokens = []\n",
    "    \n",
    "    for sample in dataset:\n",
    "        # directly call tokenizer\n",
    "        tokens = [token.text for token in tokenizer(sample)]\n",
    "        token_dataset.append(tokens)\n",
    "        all_tokens += tokens\n",
    "\n",
    "    return token_dataset, all_tokens\n",
    "\n",
    "print (\"Tokenizing val data\")\n",
    "val_data_tokens_raw, _ = tokenize_dataset_raw(val_data)\n",
    "pkl.dump(val_data_tokens_raw, open(\"val_data_tokens_raw.p\", \"wb\"))\n",
    "\n",
    "# test set tokens\n",
    "print (\"Tokenizing test data\")\n",
    "test_data_tokens_raw, _ = tokenize_dataset_raw(test_data)\n",
    "pkl.dump(test_data_tokens_raw, open(\"test_data_tokens_raw.p\", \"wb\"))\n",
    "\n",
    "# train set tokens\n",
    "print (\"Tokenizing train data\")\n",
    "train_data_tokens_raw, all_train_tokens_raw = tokenize_dataset_raw(train_sub_data)\n",
    "pkl.dump(train_data_tokens_raw, open(\"train_data_tokens_raw.p\", \"wb\"))\n",
    "pkl.dump(all_train_tokens_raw, open(\"all_train_tokens_raw.p\", \"wb\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset size is 20000\n",
      "Val dataset size is 5000\n",
      "Test dataset size is 25000\n",
      "Total number of tokens in train dataset is 4817127\n"
     ]
    }
   ],
   "source": [
    "import pickle as pkl\n",
    "# First, download datasets from here\n",
    "# Use your NYU account\n",
    "#https://drive.google.com/open?id=1eR2LFI5MGliHlaL1S2nsX4ouIO1k_ip2\n",
    "#https://drive.google.com/open?id=133QCWbiz_Xc7Qm4r6t-fJP1K669xjNlM\n",
    "#https://drive.google.com/open?id=1SuUIUpJ1iznU707ktkpnEGSwt_XIqOYp\n",
    "#https://drive.google.com/open?id=1UQsrZ2LVfcxdxxa47344fMs_qvya72KR\n",
    "\n",
    "# Then, load preprocessed train, val and test datasets\n",
    "train_data_tokens = pkl.load(open(\"train_data_tokens.p\", \"rb\"))\n",
    "all_train_tokens = pkl.load(open(\"all_train_tokens.p\", \"rb\"))\n",
    "\n",
    "train_data_tokens_raw = pkl.load(open(\"train_data_tokens_raw.p\", \"rb\"))\n",
    "all_train_tokens_raw = pkl.load(open(\"all_train_tokens_raw.p\", \"rb\"))\n",
    "\n",
    "train_target = pkl.load(open(\"train_target.p\", \"rb\"))\n",
    "\n",
    "val_data_tokens = pkl.load(open(\"val_data_tokens.p\", \"rb\"))\n",
    "val_data_tokens_raw = pkl.load(open(\"val_data_tokens_raw.p\", \"rb\"))\n",
    "val_target = pkl.load(open(\"val_target.p\", \"rb\"))\n",
    "\n",
    "#test_data_tokens = pkl.load(open(\"test_data_tokens.p\", \"rb\"))\n",
    "#test_data_tokens_raw = pkl.load(open(\"test_data_tokens_raw.p\", \"rb\"))\n",
    "#test_target = pkl.load(open(\"test_target.p\", \"rb\"))\n",
    "\n",
    "\n",
    "# double checking\n",
    "print (\"Train dataset size is {}\".format(len(train_data_tokens)))\n",
    "print (\"Val dataset size is {}\".format(len(val_data_tokens)))\n",
    "print (\"Test dataset size is {}\".format(len(test_data_tokens)))\n",
    "\n",
    "print (\"Total number of tokens in train dataset is {}\".format(len(all_train_tokens)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we are going to create the vocabulary of most common 10,000 tokens in the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build n gram tokens\n",
    "def n_gram_token(tokens, n):\n",
    "    new_tokens = []\n",
    "    for idx, word in enumerate(tokens):\n",
    "        if idx < len(tokens)-(n-1):\n",
    "            for i in range(1,n):\n",
    "                word += (\" \"+tokens[i+idx])\n",
    "            new_tokens += [word]\n",
    "    return new_tokens\n",
    "\n",
    "# return list of tokens that contains tokens for i<=n \n",
    "def get_n_gram_tokens(n, tokens):\n",
    "    list_of_n_tokens = []\n",
    "    for i in range(2,n+1):\n",
    "        list_of_n_tokens.append([n_gram_token(token_i,i) for token_i in tokens])\n",
    "    return list_of_n_tokens\n",
    "\n",
    "# get token list\n",
    "train_token_list = get_n_gram_tokens(4, train_data_tokens)\n",
    "val_token_list = get_n_gram_tokens(4, val_data_tokens)\n",
    "#test_token_list = get_n_gram_tokens(4, test_data_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build all training token based on n-gram\n",
    "def build_all_token_list(n, original_tokens, train_tokens=train_token_list):\n",
    "    all_tr_token=[]\n",
    "    for i in range(n-1):\n",
    "        for tr_token in train_tokens[i]:\n",
    "            all_tr_token += tr_token\n",
    "    return original_tokens+all_tr_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "max_vocab_size = 10000\n",
    "# save index 0 for unk and 1 for pad\n",
    "PAD_IDX = 0\n",
    "UNK_IDX = 1\n",
    "\n",
    "def build_vocab(n, original_tokens,max_vocab_size):\n",
    "    # Returns:\n",
    "    # id2token: list of tokens, where id2token[i] returns token that corresponds to token i\n",
    "    # token2id: dictionary where keys represent tokens and corresponding values represent indices\n",
    "    all_tokens = build_all_token_list(n, original_tokens)\n",
    "    token_counter = Counter(all_tokens)\n",
    "    vocab, count = zip(*token_counter.most_common(max_vocab_size))\n",
    "    id2token = list(vocab)\n",
    "    token2id = dict(zip(vocab, range(2,2+len(vocab)))) \n",
    "    id2token = ['<pad>', '<unk>'] + id2token\n",
    "    token2id['<pad>'] = PAD_IDX \n",
    "    token2id['<unk>'] = UNK_IDX\n",
    "    return token2id, id2token\n",
    "\n",
    "#token2id, id2token = build_vocab(all_train_tokens)\n",
    "#token2id, id2token = build_vocab(all_tr_token)\n",
    "#token2id, id2token = build_vocab(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token id 3292 ; token mine\n",
      "Token mine; token id 3292\n"
     ]
    }
   ],
   "source": [
    "# Lets check the dictionary by loading random token from it\n",
    "import random\n",
    "random_token_id = random.randint(0, len(id2token)-1)\n",
    "random_token = id2token[random_token_id]\n",
    "\n",
    "print (\"Token id {} ; token {}\".format(random_token_id, id2token[random_token_id]))\n",
    "print (\"Token {}; token id {}\".format(random_token, token2id[random_token]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "max_vocab_size = 10000\n",
    "# save index 0 for unk and 1 for pad\n",
    "PAD_IDX = 0\n",
    "UNK_IDX = 1\n",
    "\n",
    "def build_vocab(n, original_tokens,max_vocab_size):\n",
    "    # Returns:\n",
    "    # id2token: list of tokens, where id2token[i] returns token that corresponds to token i\n",
    "    # token2id: dictionary where keys represent tokens and corresponding values represent indices\n",
    "    all_tokens = build_all_token_list(n, original_tokens)\n",
    "    token_counter = Counter(all_tokens)\n",
    "    vocab, count = zip(*token_counter.most_common(max_vocab_size))\n",
    "    id2token = list(vocab)\n",
    "    token2id = dict(zip(vocab, range(2,2+len(vocab)))) \n",
    "    id2token = ['<pad>', '<unk>'] + id2token\n",
    "    token2id['<pad>'] = PAD_IDX \n",
    "    token2id['<unk>'] = UNK_IDX\n",
    "    return token2id, id2token\n",
    "\n",
    "\n",
    "\n",
    "# convert token to id in the dataset\n",
    "def token2index_dataset(tokens_data, token2id):\n",
    "    indices_data = []\n",
    "    for tokens in tokens_data:\n",
    "        index_list = [token2id[token] if token in token2id else UNK_IDX for token in tokens]\n",
    "        indices_data.append(index_list)\n",
    "    return indices_data\n",
    "\n",
    "#train_data_indices = token2index_dataset(train_tokens)\n",
    "#val_data_indices = token2index_dataset(val_tokens)\n",
    "#test_data_indices = token2index_dataset(test_tokens)\n",
    "\n",
    "# double checking\n",
    "#print (\"Train dataset size is {}\".format(len(train_data_indices)))\n",
    "#print (\"Val dataset size is {}\".format(len(val_data_indices)))\n",
    "#print (\"Test dataset size is {}\".format(len(test_data_indices)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are going to create PyTorch DataLoader "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_SENTENCE_LENGTH = 200\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class NewsGroupDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Class that represents a train/validation/test dataset that's readable for PyTorch\n",
    "    Note that this class inherits torch.utils.data.Dataset\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, data_list, target_list):\n",
    "        \"\"\"\n",
    "        @param data_list: list of newsgroup tokens \n",
    "        @param target_list: list of newsgroup targets \n",
    "\n",
    "        \"\"\"\n",
    "        self.data_list = data_list\n",
    "        self.target_list = target_list\n",
    "        assert (len(self.data_list) == len(self.target_list))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_list)\n",
    "        \n",
    "    def __getitem__(self, key):\n",
    "        \"\"\"\n",
    "        Triggered when you call dataset[i]\n",
    "        \"\"\"\n",
    "        \n",
    "        token_idx = self.data_list[key][:MAX_SENTENCE_LENGTH]\n",
    "        label = self.target_list[key]\n",
    "        return [token_idx, len(token_idx), label]\n",
    "\n",
    "def newsgroup_collate_func(batch):\n",
    "    \"\"\"\n",
    "    Customized function for DataLoader that dynamically pads the batch so that all \n",
    "    data have the same length\n",
    "    \"\"\"\n",
    "    data_list = []\n",
    "    label_list = []\n",
    "    length_list = []\n",
    "    #print(\"collate batch: \", batch[0][0])\n",
    "    #batch[0][0] = batch[0][0][:MAX_SENTENCE_LENGTH]\n",
    "    for datum in batch:\n",
    "        label_list.append(datum[2])\n",
    "        length_list.append(datum[1])\n",
    "    # padding\n",
    "    for datum in batch:\n",
    "        padded_vec = np.pad(np.array(datum[0]), \n",
    "                                pad_width=((0,MAX_SENTENCE_LENGTH-datum[1])), \n",
    "                                mode=\"constant\", constant_values=0)\n",
    "        data_list.append(padded_vec)\n",
    "    return [torch.from_numpy(np.array(data_list)), torch.LongTensor(length_list), torch.LongTensor(label_list)]\n",
    "\n",
    "# create pytorch dataloader\n",
    "#train_loader = NewsGroupDataset(train_data_indices, train_targets)\n",
    "#val_loader = NewsGroupDataset(val_data_indices, val_targets)\n",
    "#test_loader = NewsGroupDataset(test_data_indices, test_targets)\n",
    "\n",
    "# BATCH_SIZE = 32\n",
    "# train_dataset = NewsGroupDataset(train_data_indices, train_sub_target)\n",
    "# train_loader = torch.utils.data.DataLoader(dataset=train_dataset, \n",
    "#                                            batch_size=BATCH_SIZE,\n",
    "#                                            collate_fn=newsgroup_collate_func,\n",
    "#                                            shuffle=True)\n",
    "\n",
    "# val_dataset = NewsGroupDataset(val_data_indices, val_target)\n",
    "# val_loader = torch.utils.data.DataLoader(dataset=val_dataset, \n",
    "#                                            batch_size=BATCH_SIZE,\n",
    "#                                            collate_fn=newsgroup_collate_func,\n",
    "#                                            shuffle=True)\n",
    "\n",
    "# test_dataset = NewsGroupDataset(test_data_indices, test_target)\n",
    "# test_loader = torch.utils.data.DataLoader(dataset=test_dataset, \n",
    "#                                            batch_size=BATCH_SIZE,\n",
    "#                                            collate_fn=newsgroup_collate_func,\n",
    "#                                            shuffle=False)\n",
    "\n",
    "#for i, (data, lengths, labels) in enumerate(train_loader):\n",
    "#    print (data)\n",
    "#    print (labels)\n",
    "#    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we will define Bag-of-Words model in PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First import torch related libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class BagOfWords(nn.Module):\n",
    "    \"\"\"\n",
    "    BagOfWords classification model\n",
    "    \"\"\"\n",
    "    def __init__(self, vocab_size, emb_dim):\n",
    "        \"\"\"\n",
    "        @param vocab_size: size of the vocabulary. \n",
    "        @param emb_dim: size of the word embedding; 100 at least\n",
    "        \"\"\"\n",
    "        super(BagOfWords, self).__init__()\n",
    "        # pay attention to padding_idx \n",
    "        self.embed = nn.Embedding(vocab_size, emb_dim, padding_idx=0)\n",
    "        self.linear = nn.Linear(emb_dim,20)\n",
    "    \n",
    "    def forward(self, data, length):\n",
    "        \"\"\"\n",
    "        \n",
    "        @param data: matrix of size (batch_size, max_sentence_length). Each row in data represents a \n",
    "            review that is represented using n-gram index. Note that they are padded to have same length.\n",
    "        @param length: an int tensor of size (batch_size), which represents the non-trivial (excludes padding)\n",
    "            length of each sentences in the data.\n",
    "        \"\"\"\n",
    "        out = self.embed(data)\n",
    "        out = torch.sum(out, dim=1)\n",
    "        out /= length.view(length.size()[0],1).expand_as(out).float()\n",
    "     \n",
    "        # return logits\n",
    "        out = self.linear(out.float())\n",
    "        return out\n",
    "\n",
    "#emb_dim =100\n",
    "#model = BagOfWords(len(id2token), emb_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# learning_rate = 0.01\n",
    "# num_epochs = 10 # number epoch to train\n",
    "\n",
    "# # Criterion and Optimizer\n",
    "# criterion = torch.nn.CrossEntropyLoss()  \n",
    "# optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "# #optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "\n",
    "# Function for testing the model\n",
    "def test_model(loader, model):\n",
    "    \"\"\"\n",
    "    Help function that tests the model's performance on a dataset\n",
    "    @param: loader - data loader for the dataset to test against\n",
    "    \"\"\"\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    model.eval()\n",
    "    for data, lengths, labels in loader:\n",
    "        data_batch, length_batch, label_batch = data, lengths, labels\n",
    "        outputs = F.softmax(model(data_batch, length_batch), dim=1)\n",
    "        predicted = outputs.max(1, keepdim=True)[1]\n",
    "        \n",
    "        total += labels.size(0)\n",
    "        correct += predicted.eq(labels.view_as(predicted)).sum().item()\n",
    "    return (100 * correct / total)\n",
    "\n",
    "# for epoch in range(num_epochs):\n",
    "#     for i, (data, lengths, labels) in enumerate(train_loader):\n",
    "#         model.train()\n",
    "#         data_batch, length_batch, label_batch = data, lengths, labels\n",
    "#         optimizer.zero_grad()\n",
    "#         outputs = model(data_batch, length_batch)\n",
    "#         loss = criterion(outputs, label_batch)\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "#         # validate every 100 iterations\n",
    "#         if i > 0 and i % 100 == 0:\n",
    "#             # validate\n",
    "#             val_acc = test_model(val_loader, model)\n",
    "#             print('Epoch: [{}/{}], Step: [{}/{}], Validation Acc: {}'.format( \n",
    "#                        epoch+1, num_epochs, i+1, len(train_loader), val_acc))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tune parameters for n gram; will try all i such that 1<=i<=n grams\n",
    "MAX_SENTENCE_LENGTH = 200\n",
    "\n",
    "def run_model(emb_dim, learning_rate, num_epochs, train_loader, val_loader, id2token):\n",
    "    \n",
    "    model = BagOfWords(len(id2token), emb_dim)\n",
    "    criterion = torch.nn.CrossEntropyLoss()  \n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "        \n",
    "    for epoch in range(num_epochs):\n",
    "            for i, (data, lengths, labels) in enumerate(train_loader):\n",
    "                model.train()\n",
    "                data_batch, length_batch, label_batch = data, lengths, labels\n",
    "                optimizer.zero_grad()\n",
    "                outputs = model(data_batch, length_batch)\n",
    "                loss = criterion(outputs, label_batch)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                if i > 0 and i % 100 == 0:\n",
    "                    # validate\n",
    "                    val_acc = test_model(val_loader, model)\n",
    "                    print('Epoch: [{}/{}], Step: [{}/{}], Validation Acc: {}'.format( \n",
    "                       epoch+1, num_epochs, i+1, len(train_loader), val_acc))\n",
    "\n",
    "\n",
    "            \n",
    "            # print accuracy every epoch\n",
    "            \n",
    "    val_acc_i = test_model(val_loader, model)\n",
    "    return val_acc_i\n",
    "        \n",
    "# set default\n",
    "max_vocab_default = 20000\n",
    "emb_dim_default = 200\n",
    "lr_default = 0.01\n",
    "num_epochs_default = 10\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "    \n",
    "def tune_n_gram(n_size):    \n",
    "    # tune n-gram\n",
    "    print(\"Tuning n-gram:\")\n",
    "    for n_i in range(1,n_size+1):\n",
    "        # build vocab\n",
    "        token2id, id2token = build_vocab(n_i, all_train_tokens,max_vocab_default)\n",
    "        if n_i == 1:\n",
    "            new_train = train_data_tokens.copy()\n",
    "            new_val = val_data_tokens.copy()\n",
    "        else:\n",
    "            for idx in range(train_size):\n",
    "                new_train[idx] += train_token_list[n_i-2][idx]\n",
    "            for idx in range(val_size):\n",
    "                new_val[idx] += val_token_list[n_i -2][idx]\n",
    "                \n",
    "        print(new_train[0][len(new_train[0])-1])\n",
    "        print(new_val[0][len(new_val[0])-1])\n",
    "        # get indices\n",
    "        train_data_indices = token2index_dataset(new_train,token2id)\n",
    "        val_data_indices = token2index_dataset(new_val, token2id)\n",
    "        \n",
    "        # prepare data with dataloader for model\n",
    "        train_dataset = NewsGroupDataset(train_data_indices, train_target)\n",
    "        train_loader = torch.utils.data.DataLoader(dataset=train_dataset, \n",
    "                                               batch_size=BATCH_SIZE,\n",
    "                                               collate_fn=newsgroup_collate_func,\n",
    "                                               shuffle=True)\n",
    "        \n",
    "        val_dataset = NewsGroupDataset(val_data_indices, val_target)\n",
    "        val_loader = torch.utils.data.DataLoader(dataset=val_dataset, \n",
    "                                                   batch_size=BATCH_SIZE,\n",
    "                                                   collate_fn=newsgroup_collate_func,\n",
    "                                                   shuffle=True)\n",
    "        \n",
    "        # create model\n",
    "        #emb_dim =100\n",
    "        #model = BagOfWords(len(id2token), emb_dim)\n",
    "        #learning_rate = 0.01\n",
    "        #num_epochs = 10 # number epoch to train\n",
    "\n",
    "        # Criterion and Optimizer\n",
    "        #criterion = torch.nn.CrossEntropyLoss()  \n",
    "        #optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "        \n",
    "        # run\n",
    "        print(\"n-gram: \", n_i)\n",
    "        #for epoch in range(num_epochs):\n",
    "        #    for i, (data, lengths, labels) in enumerate(train_loader):\n",
    "        #        model.train()\n",
    "        #        data_batch, length_batch, label_batch = data, lengths, labels\n",
    "        #        optimizer.zero_grad()\n",
    "        #        outputs = model(data_batch, length_batch)\n",
    "        #        loss = criterion(outputs, label_batch)\n",
    "        #        loss.backward()\n",
    "        #        optimizer.step()\n",
    "        #    \n",
    "        #    # print accuracy every epoch\n",
    "        #    val_acc = test_model(val_loader, model)\n",
    "        #    print('Epoch: [{}/{}], Validation Acc: {}'.format( \n",
    "        #                epoch+1, num_epochs, val_acc))\n",
    "        #val_acc_i = test_model(val_loader, model)\n",
    "        \n",
    "        \n",
    "        val_acc_i = run_model(emb_dim_default, lr_default, num_epochs_default, train_loader, val_loader,id2token)\n",
    "        print (\"Val Acc {}\".format(val_acc_i))\n",
    "        if val_acc_i > max_val_acc:\n",
    "            max_val_acc = val_acc_i\n",
    "            best_n = n_i\n",
    "            \n",
    "    return best_n\n",
    "    \n",
    "    \n",
    "    \n",
    "    # update train data and val according to best n-gram size\n",
    "    for i in range(1,best_n+1):\n",
    "        for idx in range(train_size):\n",
    "            train_data_tokens[idx]+= train_token_list[i-2][idx]\n",
    "        for idx in range(val_size):\n",
    "            val_data_tokens[idx] += val_token_list[i-2][idx]\n",
    "            \n",
    "    \n",
    "    \n",
    "    # tune vocabulary size\n",
    "    best_vocab_size = -1\n",
    "    max_val_acc = -1\n",
    "    \n",
    "    print(\"Tuning max vocabulary size:\")\n",
    "    for v_s in vocab_size_list:\n",
    "        print(\"max vocab size: \", v_s)\n",
    "        token2id, id2token = build_vocab(best_n, v_s)\n",
    "        \n",
    "        # get indices\n",
    "        train_data_indices = token2index_dataset(train_data_tokens, token2id)\n",
    "        val_data_indices = token2index_dataset(val_data_tokens, token2id)\n",
    "    \n",
    "        # prepare data with dataloader for model\n",
    "        train_dataset = NewsGroupDataset(train_data_indices, train_target)\n",
    "        train_loader = torch.utils.data.DataLoader(dataset=train_dataset, \n",
    "                                               batch_size=BATCH_SIZE,\n",
    "                                               collate_fn=newsgroup_collate_func,\n",
    "                                               shuffle=True)\n",
    "        \n",
    "        val_dataset = NewsGroupDataset(val_data_indices, val_target)\n",
    "        val_loader = torch.utils.data.DataLoader(dataset=val_dataset, \n",
    "                                                   batch_size=BATCH_SIZE,\n",
    "                                                   collate_fn=newsgroup_collate_func,\n",
    "                                                   shuffle=True)\n",
    "        \n",
    "        \n",
    "        \n",
    "        val_acc_i = run_model(emb_dim_default, lr_default, num_epochs_default, train_loader, val_loader, id2token)\n",
    "        print (\"Val Acc {}\".format(val_acc_i))\n",
    "        if val_acc_i > max_val_acc:\n",
    "            max_val_acc = val_acc_i\n",
    "            best_vocab_size = v_s\n",
    "    \n",
    "    # update best vocab size\n",
    "    token2id, id2token = build_vocab(best_n, best_vocab_size)\n",
    "    # get indices\n",
    "    train_data_indices = token2index_dataset(train_data_tokens, token2id)\n",
    "    val_data_indices = token2index_dataset(val_data_tokens, token2id)\n",
    "    \n",
    "    # prepare data with dataloader for model\n",
    "    train_dataset = NewsGroupDataset(train_data_indices, train_target)\n",
    "    train_loader = torch.utils.data.DataLoader(dataset=train_dataset, \n",
    "                                               batch_size=BATCH_SIZE,\n",
    "                                               collate_fn=newsgroup_collate_func,\n",
    "                                               shuffle=True)\n",
    "        \n",
    "    val_dataset = NewsGroupDataset(val_data_indices, val_target)\n",
    "    val_loader = torch.utils.data.DataLoader(dataset=val_dataset, \n",
    "                                                   batch_size=BATCH_SIZE,\n",
    "                                                   collate_fn=newsgroup_collate_func,\n",
    "                                                   shuffle=True)\n",
    "        \n",
    "    \n",
    "    # tune embedding dimension\n",
    "    best_emb_dim = -1\n",
    "    max_val_acc = -1\n",
    "    \n",
    "    print(\"Tuning embedding dimension:\")\n",
    "    for emb_i in emb_dim_list:\n",
    "        print(\"embedding dim: \", emb_i)\n",
    "        val_acc_i = run_model(emb_i, lr_default, num_epochs_default, train_loader, val_loader, id2token)\n",
    "        print (\"Val Acc {}\".format(val_acc_i))\n",
    "        if val_acc_i > max_val_acc:\n",
    "            max_val_acc = val_acc_i\n",
    "            best_emb_dim = emb_i\n",
    "    \n",
    "    return [best_n, best_vocab_size, best_emb_dim]\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tune tokenization\n",
      "with post-processing\n",
      "Epoch: [1/10], Step: [101/313], Validation Acc: 84.52\n",
      "Epoch: [1/10], Step: [201/313], Validation Acc: 87.22\n",
      "Epoch: [1/10], Step: [301/313], Validation Acc: 88.26\n",
      "Epoch: [2/10], Step: [101/313], Validation Acc: 87.72\n",
      "Epoch: [2/10], Step: [201/313], Validation Acc: 87.96\n",
      "Epoch: [2/10], Step: [301/313], Validation Acc: 87.74\n",
      "Epoch: [3/10], Step: [101/313], Validation Acc: 87.1\n",
      "Epoch: [3/10], Step: [201/313], Validation Acc: 87.08\n",
      "Epoch: [3/10], Step: [301/313], Validation Acc: 86.74\n",
      "Epoch: [4/10], Step: [101/313], Validation Acc: 86.7\n",
      "Epoch: [4/10], Step: [201/313], Validation Acc: 86.68\n",
      "Epoch: [4/10], Step: [301/313], Validation Acc: 86.62\n",
      "Epoch: [5/10], Step: [101/313], Validation Acc: 86.26\n",
      "Epoch: [5/10], Step: [201/313], Validation Acc: 86.52\n",
      "Epoch: [5/10], Step: [301/313], Validation Acc: 85.88\n",
      "Epoch: [6/10], Step: [101/313], Validation Acc: 85.98\n",
      "Epoch: [6/10], Step: [201/313], Validation Acc: 86.24\n",
      "Epoch: [6/10], Step: [301/313], Validation Acc: 86.52\n",
      "Epoch: [7/10], Step: [101/313], Validation Acc: 85.94\n",
      "Epoch: [7/10], Step: [201/313], Validation Acc: 86.52\n",
      "Epoch: [7/10], Step: [301/313], Validation Acc: 86.16\n",
      "Epoch: [8/10], Step: [101/313], Validation Acc: 86.14\n",
      "Epoch: [8/10], Step: [201/313], Validation Acc: 86.18\n",
      "Epoch: [8/10], Step: [301/313], Validation Acc: 86.24\n",
      "Epoch: [9/10], Step: [101/313], Validation Acc: 86.22\n",
      "Epoch: [9/10], Step: [201/313], Validation Acc: 86.32\n",
      "Epoch: [9/10], Step: [301/313], Validation Acc: 86.2\n",
      "Epoch: [10/10], Step: [101/313], Validation Acc: 86.26\n",
      "Epoch: [10/10], Step: [201/313], Validation Acc: 86.22\n",
      "Epoch: [10/10], Step: [301/313], Validation Acc: 86.2\n",
      "Val Acc 86.18\n",
      "without post-processing\n",
      "Epoch: [1/10], Step: [101/313], Validation Acc: 83.76\n",
      "Epoch: [1/10], Step: [201/313], Validation Acc: 86.76\n",
      "Epoch: [1/10], Step: [301/313], Validation Acc: 87.98\n",
      "Epoch: [2/10], Step: [101/313], Validation Acc: 87.14\n",
      "Epoch: [2/10], Step: [201/313], Validation Acc: 86.86\n",
      "Epoch: [2/10], Step: [301/313], Validation Acc: 87.82\n",
      "Epoch: [3/10], Step: [101/313], Validation Acc: 87.5\n",
      "Epoch: [3/10], Step: [201/313], Validation Acc: 87.26\n",
      "Epoch: [3/10], Step: [301/313], Validation Acc: 86.82\n",
      "Epoch: [4/10], Step: [101/313], Validation Acc: 86.82\n",
      "Epoch: [4/10], Step: [201/313], Validation Acc: 86.38\n",
      "Epoch: [4/10], Step: [301/313], Validation Acc: 86.46\n",
      "Epoch: [5/10], Step: [101/313], Validation Acc: 86.22\n",
      "Epoch: [5/10], Step: [201/313], Validation Acc: 85.82\n",
      "Epoch: [5/10], Step: [301/313], Validation Acc: 85.64\n",
      "Epoch: [6/10], Step: [101/313], Validation Acc: 85.78\n",
      "Epoch: [6/10], Step: [201/313], Validation Acc: 85.94\n",
      "Epoch: [6/10], Step: [301/313], Validation Acc: 85.76\n",
      "Epoch: [7/10], Step: [101/313], Validation Acc: 85.96\n",
      "Epoch: [7/10], Step: [201/313], Validation Acc: 86.0\n",
      "Epoch: [7/10], Step: [301/313], Validation Acc: 85.76\n",
      "Epoch: [8/10], Step: [101/313], Validation Acc: 85.96\n",
      "Epoch: [8/10], Step: [201/313], Validation Acc: 85.9\n",
      "Epoch: [8/10], Step: [301/313], Validation Acc: 85.98\n",
      "Epoch: [9/10], Step: [101/313], Validation Acc: 85.98\n",
      "Epoch: [9/10], Step: [201/313], Validation Acc: 85.9\n",
      "Epoch: [9/10], Step: [301/313], Validation Acc: 85.88\n",
      "Epoch: [10/10], Step: [101/313], Validation Acc: 85.76\n",
      "Epoch: [10/10], Step: [201/313], Validation Acc: 85.88\n",
      "Epoch: [10/10], Step: [301/313], Validation Acc: 85.98\n",
      "Val Acc 86.02\n"
     ]
    }
   ],
   "source": [
    "# tune tokenization\n",
    "print(\"Tune tokenization\")\n",
    "print(\"with post-processing\")\n",
    "token2id, id2token = build_vocab(1, all_train_tokens,max_vocab_default)\n",
    "train_data_indices = token2index_dataset(train_data_tokens,token2id)\n",
    "val_data_indices = token2index_dataset(val_data_tokens, token2id)\n",
    "\n",
    "# prepare data with dataloader for model\n",
    "train_dataset = NewsGroupDataset(train_data_indices, train_target)\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, \n",
    "                                       batch_size=BATCH_SIZE,\n",
    "                                       collate_fn=newsgroup_collate_func,\n",
    "                                       shuffle=True)\n",
    "\n",
    "val_dataset = NewsGroupDataset(val_data_indices, val_target)\n",
    "val_loader = torch.utils.data.DataLoader(dataset=val_dataset, \n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           collate_fn=newsgroup_collate_func,\n",
    "                                           shuffle=True)\n",
    "# run\n",
    "val_acc_i = run_model(emb_dim_default, lr_default, num_epochs_default, train_loader, val_loader,id2token)\n",
    "print (\"Val Acc {}\".format(val_acc_i))\n",
    "\n",
    "\n",
    "print(\"without post-processing\")\n",
    "token2id, id2token = build_vocab(1, all_train_tokens_raw,max_vocab_default)\n",
    "train_data_indices = token2index_dataset(train_data_tokens_raw,token2id)\n",
    "val_data_indices = token2index_dataset(val_data_tokens_raw, token2id)\n",
    "\n",
    "# prepare data with dataloader for model\n",
    "train_dataset = NewsGroupDataset(train_data_indices, train_target)\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, \n",
    "                                       batch_size=BATCH_SIZE,\n",
    "                                       collate_fn=newsgroup_collate_func,\n",
    "                                       shuffle=True)\n",
    "\n",
    "val_dataset = NewsGroupDataset(val_data_indices, val_target)\n",
    "val_loader = torch.utils.data.DataLoader(dataset=val_dataset, \n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           collate_fn=newsgroup_collate_func,\n",
    "                                           shuffle=True)\n",
    "# run\n",
    "val_acc_i = run_model(emb_dim_default, lr_default, num_epochs_default, train_loader, val_loader,id2token)\n",
    "print (\"Val Acc {}\".format(val_acc_i))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tuning n-gram:\n",
      "n-gram:  1\n",
      "Epoch: [1/10], Step: [101/313], Validation Acc: 83.94\n",
      "Epoch: [1/10], Step: [201/313], Validation Acc: 86.1\n",
      "Epoch: [1/10], Step: [301/313], Validation Acc: 88.3\n",
      "Epoch: [2/10], Step: [101/313], Validation Acc: 87.98\n",
      "Epoch: [2/10], Step: [201/313], Validation Acc: 88.16\n",
      "Epoch: [2/10], Step: [301/313], Validation Acc: 87.8\n",
      "Epoch: [3/10], Step: [101/313], Validation Acc: 87.3\n",
      "Epoch: [3/10], Step: [201/313], Validation Acc: 87.38\n",
      "Epoch: [3/10], Step: [301/313], Validation Acc: 87.4\n",
      "Epoch: [4/10], Step: [101/313], Validation Acc: 87.32\n",
      "Epoch: [4/10], Step: [201/313], Validation Acc: 87.26\n",
      "Epoch: [4/10], Step: [301/313], Validation Acc: 86.36\n",
      "Epoch: [5/10], Step: [101/313], Validation Acc: 86.72\n",
      "Epoch: [5/10], Step: [201/313], Validation Acc: 85.94\n",
      "Epoch: [5/10], Step: [301/313], Validation Acc: 86.38\n",
      "Epoch: [6/10], Step: [101/313], Validation Acc: 86.16\n",
      "Epoch: [6/10], Step: [201/313], Validation Acc: 86.02\n",
      "Epoch: [6/10], Step: [301/313], Validation Acc: 86.16\n",
      "Epoch: [7/10], Step: [101/313], Validation Acc: 86.04\n",
      "Epoch: [7/10], Step: [201/313], Validation Acc: 86.12\n",
      "Epoch: [7/10], Step: [301/313], Validation Acc: 86.02\n",
      "Epoch: [8/10], Step: [101/313], Validation Acc: 86.06\n",
      "Epoch: [8/10], Step: [201/313], Validation Acc: 86.34\n",
      "Epoch: [8/10], Step: [301/313], Validation Acc: 86.02\n",
      "Epoch: [9/10], Step: [101/313], Validation Acc: 86.06\n",
      "Epoch: [9/10], Step: [201/313], Validation Acc: 86.1\n",
      "Epoch: [9/10], Step: [301/313], Validation Acc: 86.14\n",
      "Epoch: [10/10], Step: [101/313], Validation Acc: 86.06\n",
      "Epoch: [10/10], Step: [201/313], Validation Acc: 86.04\n",
      "Epoch: [10/10], Step: [301/313], Validation Acc: 86.14\n",
      "Val Acc 86.04\n",
      "n-gram:  2\n",
      "Epoch: [1/10], Step: [101/313], Validation Acc: 82.46\n",
      "Epoch: [1/10], Step: [201/313], Validation Acc: 86.66\n",
      "Epoch: [1/10], Step: [301/313], Validation Acc: 85.44\n",
      "Epoch: [2/10], Step: [101/313], Validation Acc: 87.22\n",
      "Epoch: [2/10], Step: [201/313], Validation Acc: 87.26\n",
      "Epoch: [2/10], Step: [301/313], Validation Acc: 86.8\n",
      "Epoch: [3/10], Step: [101/313], Validation Acc: 86.68\n",
      "Epoch: [3/10], Step: [201/313], Validation Acc: 86.32\n",
      "Epoch: [3/10], Step: [301/313], Validation Acc: 86.4\n",
      "Epoch: [4/10], Step: [101/313], Validation Acc: 85.6\n",
      "Epoch: [4/10], Step: [201/313], Validation Acc: 85.58\n",
      "Epoch: [4/10], Step: [301/313], Validation Acc: 84.94\n",
      "Epoch: [5/10], Step: [101/313], Validation Acc: 85.52\n",
      "Epoch: [5/10], Step: [201/313], Validation Acc: 85.06\n",
      "Epoch: [5/10], Step: [301/313], Validation Acc: 84.98\n",
      "Epoch: [6/10], Step: [101/313], Validation Acc: 84.98\n",
      "Epoch: [6/10], Step: [201/313], Validation Acc: 84.84\n",
      "Epoch: [6/10], Step: [301/313], Validation Acc: 83.86\n",
      "Epoch: [7/10], Step: [101/313], Validation Acc: 84.5\n",
      "Epoch: [7/10], Step: [201/313], Validation Acc: 84.34\n",
      "Epoch: [7/10], Step: [301/313], Validation Acc: 84.24\n",
      "Epoch: [8/10], Step: [101/313], Validation Acc: 84.34\n",
      "Epoch: [8/10], Step: [201/313], Validation Acc: 84.36\n",
      "Epoch: [8/10], Step: [301/313], Validation Acc: 84.16\n",
      "Epoch: [9/10], Step: [101/313], Validation Acc: 84.22\n",
      "Epoch: [9/10], Step: [201/313], Validation Acc: 84.18\n",
      "Epoch: [9/10], Step: [301/313], Validation Acc: 83.96\n",
      "Epoch: [10/10], Step: [101/313], Validation Acc: 84.26\n",
      "Epoch: [10/10], Step: [201/313], Validation Acc: 84.28\n",
      "Epoch: [10/10], Step: [301/313], Validation Acc: 84.44\n",
      "Val Acc 84.32\n",
      "n-gram:  3\n",
      "Epoch: [1/10], Step: [101/313], Validation Acc: 83.4\n",
      "Epoch: [1/10], Step: [201/313], Validation Acc: 86.48\n",
      "Epoch: [1/10], Step: [301/313], Validation Acc: 87.48\n",
      "Epoch: [2/10], Step: [101/313], Validation Acc: 87.2\n",
      "Epoch: [2/10], Step: [201/313], Validation Acc: 87.04\n",
      "Epoch: [2/10], Step: [301/313], Validation Acc: 86.16\n",
      "Epoch: [3/10], Step: [101/313], Validation Acc: 85.22\n",
      "Epoch: [3/10], Step: [201/313], Validation Acc: 86.0\n",
      "Epoch: [3/10], Step: [301/313], Validation Acc: 86.0\n",
      "Epoch: [4/10], Step: [101/313], Validation Acc: 85.54\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-98-413c7845043f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     51\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mbest_n\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m \u001b[0mbest_n\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtune_n_gram\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-98-413c7845043f>\u001b[0m in \u001b[0;36mtune_n_gram\u001b[0;34m(n_size)\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"n-gram: \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_i\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m         \u001b[0mval_acc_i\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0memb_dim_default\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr_default\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs_default\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mid2token\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m         \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"Val Acc {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_acc_i\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mval_acc_i\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mmax_val_acc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-91-ace568d65ba7>\u001b[0m in \u001b[0;36mrun_model\u001b[0;34m(emb_dim, learning_rate, num_epochs, train_loader, val_loader, id2token)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlengths\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m                 \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m                 \u001b[0mdata_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlength_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlengths\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/envs/nlpclass/lib/python3.6/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    312\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_workers\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# same-process loading\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m             \u001b[0mindices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample_iter\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 314\u001b[0;31m             \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollate_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    315\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    316\u001b[0m                 \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpin_memory_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-38-5d1a0299f27e>\u001b[0m in \u001b[0;36mnewsgroup_collate_func\u001b[0;34m(batch)\u001b[0m\n\u001b[1;32m     50\u001b[0m         padded_vec = np.pad(np.array(datum[0]), \n\u001b[1;32m     51\u001b[0m                                 \u001b[0mpad_width\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mMAX_SENTENCE_LENGTH\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mdatum\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m                                 mode=\"constant\", constant_values=0)\n\u001b[0m\u001b[1;32m     53\u001b[0m         \u001b[0mdata_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpadded_vec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLongTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlength_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLongTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/envs/nlpclass/lib/python3.6/site-packages/numpy/lib/arraypad.py\u001b[0m in \u001b[0;36mpad\u001b[0;34m(array, pad_width, mode, **kwargs)\u001b[0m\n\u001b[1;32m   1237\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'end_values'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'constant_values'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1238\u001b[0m                 kwargs[i] = _normalize_shape(narray, kwargs[i],\n\u001b[0;32m-> 1239\u001b[0;31m                                              cast_to_int=False)\n\u001b[0m\u001b[1;32m   1240\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1241\u001b[0m         \u001b[0;31m# Drop back to old, slower np.apply_along_axis mode for user-supplied\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/envs/nlpclass/lib/python3.6/site-packages/numpy/lib/arraypad.py\u001b[0m in \u001b[0;36m_normalize_shape\u001b[0;34m(ndarray, shape, cast_to_int)\u001b[0m\n\u001b[1;32m    933\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    934\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 935\u001b[0;31m         \u001b[0mshape_arr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbroadcast_to\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape_arr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mndims\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    936\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    937\u001b[0m         \u001b[0mfmt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"Unable to create correctly shaped tuple from %s\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/envs/nlpclass/lib/python3.6/site-packages/numpy/lib/stride_tricks.py\u001b[0m in \u001b[0;36mbroadcast_to\u001b[0;34m(array, shape, subok)\u001b[0m\n\u001b[1;32m    174\u001b[0m            [1, 2, 3]])\n\u001b[1;32m    175\u001b[0m     \"\"\"\n\u001b[0;32m--> 176\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_broadcast_to\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubok\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msubok\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreadonly\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    177\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/envs/nlpclass/lib/python3.6/site-packages/numpy/lib/stride_tricks.py\u001b[0m in \u001b[0;36m_broadcast_to\u001b[0;34m(array, shape, subok, readonly)\u001b[0m\n\u001b[1;32m    126\u001b[0m     it = np.nditer(\n\u001b[1;32m    127\u001b[0m         \u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'multi_index'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'refs_ok'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'zerosize_ok'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mextras\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 128\u001b[0;31m         op_flags=[op_flag], itershape=shape, order='C')\n\u001b[0m\u001b[1;32m    129\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mit\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m         \u001b[0;31m# never really has writebackifcopy semantics\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def tune_n_gram(n_size):   \n",
    "    \n",
    "    new_train = []\n",
    "    new_val = []\n",
    "    max_val_acc = -1\n",
    "    best_n = -1\n",
    "    train_size = len(train_data_tokens)\n",
    "    val_size = len(val_data_tokens)\n",
    "    \n",
    "    \n",
    "    # tune n-gram\n",
    "    print(\"Tuning n-gram:\")\n",
    "    for n_i in range(1,n_size+1):\n",
    "        # build vocab\n",
    "        token2id, id2token = build_vocab(n_i, all_train_tokens,max_vocab_default)\n",
    "        if n_i == 1:\n",
    "            new_train = train_data_tokens.copy()\n",
    "            new_val = val_data_tokens.copy()\n",
    "        else:\n",
    "            for idx in range(train_size):\n",
    "                new_train[idx] += train_token_list[n_i-2][idx]\n",
    "            for idx in range(val_size):\n",
    "                new_val[idx] += val_token_list[n_i -2][idx]\n",
    "                \n",
    "        # get indices\n",
    "        train_data_indices = token2index_dataset(new_train,token2id)\n",
    "        val_data_indices = token2index_dataset(new_val, token2id)\n",
    "        \n",
    "        # prepare data with dataloader for model\n",
    "        train_dataset = NewsGroupDataset(train_data_indices, train_target)\n",
    "        train_loader = torch.utils.data.DataLoader(dataset=train_dataset, \n",
    "                                               batch_size=BATCH_SIZE,\n",
    "                                               collate_fn=newsgroup_collate_func,\n",
    "                                               shuffle=True)\n",
    "        \n",
    "        val_dataset = NewsGroupDataset(val_data_indices, val_target)\n",
    "        val_loader = torch.utils.data.DataLoader(dataset=val_dataset, \n",
    "                                                   batch_size=BATCH_SIZE,\n",
    "                                                   collate_fn=newsgroup_collate_func,\n",
    "                                                   shuffle=True)\n",
    "                \n",
    "        # run\n",
    "        print(\"n-gram: \", n_i)        \n",
    "        \n",
    "        val_acc_i = run_model(emb_dim_default, lr_default, num_epochs_default, train_loader, val_loader,id2token)\n",
    "        print (\"Val Acc {}\".format(val_acc_i))\n",
    "        if val_acc_i > max_val_acc:\n",
    "            max_val_acc = val_acc_i\n",
    "            best_n = n_i\n",
    "            \n",
    "    return best_n\n",
    "\n",
    "best_n = tune_n_gram(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tuning model hyperparameters:\n",
      "current config: \n",
      "n-gram:  1  max vocab size:  10000  embedding size:  100\n",
      "Epoch: [1/10], Step: [101/313], Validation Acc: 79.12\n",
      "Epoch: [1/10], Step: [201/313], Validation Acc: 86.0\n",
      "Epoch: [1/10], Step: [301/313], Validation Acc: 87.3\n",
      "Epoch: [2/10], Step: [101/313], Validation Acc: 87.0\n",
      "Epoch: [2/10], Step: [201/313], Validation Acc: 87.4\n",
      "Epoch: [2/10], Step: [301/313], Validation Acc: 87.9\n",
      "Epoch: [3/10], Step: [101/313], Validation Acc: 87.44\n",
      "Epoch: [3/10], Step: [201/313], Validation Acc: 86.78\n",
      "Epoch: [3/10], Step: [301/313], Validation Acc: 87.02\n",
      "Epoch: [4/10], Step: [101/313], Validation Acc: 86.84\n",
      "Epoch: [4/10], Step: [201/313], Validation Acc: 86.74\n",
      "Epoch: [4/10], Step: [301/313], Validation Acc: 86.44\n",
      "Epoch: [5/10], Step: [101/313], Validation Acc: 87.02\n",
      "Epoch: [5/10], Step: [201/313], Validation Acc: 86.36\n",
      "Epoch: [5/10], Step: [301/313], Validation Acc: 86.14\n",
      "Epoch: [6/10], Step: [101/313], Validation Acc: 85.7\n",
      "Epoch: [6/10], Step: [201/313], Validation Acc: 85.52\n",
      "Epoch: [6/10], Step: [301/313], Validation Acc: 85.72\n",
      "Epoch: [7/10], Step: [101/313], Validation Acc: 85.62\n",
      "Epoch: [7/10], Step: [201/313], Validation Acc: 85.76\n",
      "Epoch: [7/10], Step: [301/313], Validation Acc: 85.64\n",
      "Epoch: [8/10], Step: [101/313], Validation Acc: 85.36\n",
      "Epoch: [8/10], Step: [201/313], Validation Acc: 84.44\n",
      "Epoch: [8/10], Step: [301/313], Validation Acc: 85.1\n",
      "Epoch: [9/10], Step: [101/313], Validation Acc: 85.12\n",
      "Epoch: [9/10], Step: [201/313], Validation Acc: 84.9\n",
      "Epoch: [9/10], Step: [301/313], Validation Acc: 85.18\n",
      "Epoch: [10/10], Step: [101/313], Validation Acc: 84.98\n",
      "Epoch: [10/10], Step: [201/313], Validation Acc: 85.46\n",
      "Epoch: [10/10], Step: [301/313], Validation Acc: 85.04\n",
      "Val Acc 84.44\n",
      "current config: \n",
      "n-gram:  1  max vocab size:  10000  embedding size:  200\n",
      "Epoch: [1/10], Step: [101/313], Validation Acc: 81.84\n",
      "Epoch: [1/10], Step: [201/313], Validation Acc: 86.92\n",
      "Epoch: [1/10], Step: [301/313], Validation Acc: 87.42\n",
      "Epoch: [2/10], Step: [101/313], Validation Acc: 86.38\n",
      "Epoch: [2/10], Step: [201/313], Validation Acc: 87.52\n",
      "Epoch: [2/10], Step: [301/313], Validation Acc: 86.14\n",
      "Epoch: [3/10], Step: [101/313], Validation Acc: 87.14\n",
      "Epoch: [3/10], Step: [201/313], Validation Acc: 86.0\n",
      "Epoch: [3/10], Step: [301/313], Validation Acc: 86.82\n",
      "Epoch: [4/10], Step: [101/313], Validation Acc: 86.32\n",
      "Epoch: [4/10], Step: [201/313], Validation Acc: 86.58\n",
      "Epoch: [4/10], Step: [301/313], Validation Acc: 86.42\n",
      "Epoch: [5/10], Step: [101/313], Validation Acc: 85.42\n",
      "Epoch: [5/10], Step: [201/313], Validation Acc: 85.72\n",
      "Epoch: [5/10], Step: [301/313], Validation Acc: 86.22\n",
      "Epoch: [6/10], Step: [101/313], Validation Acc: 85.54\n",
      "Epoch: [6/10], Step: [201/313], Validation Acc: 85.7\n",
      "Epoch: [6/10], Step: [301/313], Validation Acc: 85.0\n",
      "Epoch: [7/10], Step: [101/313], Validation Acc: 85.36\n",
      "Epoch: [7/10], Step: [201/313], Validation Acc: 85.68\n",
      "Epoch: [7/10], Step: [301/313], Validation Acc: 85.38\n",
      "Epoch: [8/10], Step: [101/313], Validation Acc: 85.08\n",
      "Epoch: [8/10], Step: [201/313], Validation Acc: 85.04\n",
      "Epoch: [8/10], Step: [301/313], Validation Acc: 85.2\n",
      "Epoch: [9/10], Step: [101/313], Validation Acc: 85.12\n",
      "Epoch: [9/10], Step: [201/313], Validation Acc: 84.86\n",
      "Epoch: [9/10], Step: [301/313], Validation Acc: 84.94\n",
      "Epoch: [10/10], Step: [101/313], Validation Acc: 84.66\n",
      "Epoch: [10/10], Step: [201/313], Validation Acc: 83.54\n",
      "Epoch: [10/10], Step: [301/313], Validation Acc: 84.8\n",
      "Val Acc 84.62\n",
      "current config: \n",
      "n-gram:  1  max vocab size:  10000  embedding size:  500\n",
      "Epoch: [1/10], Step: [101/313], Validation Acc: 80.18\n",
      "Epoch: [1/10], Step: [201/313], Validation Acc: 86.06\n",
      "Epoch: [1/10], Step: [301/313], Validation Acc: 87.04\n",
      "Epoch: [2/10], Step: [101/313], Validation Acc: 86.24\n",
      "Epoch: [2/10], Step: [201/313], Validation Acc: 87.2\n",
      "Epoch: [2/10], Step: [301/313], Validation Acc: 86.04\n",
      "Epoch: [3/10], Step: [101/313], Validation Acc: 86.22\n",
      "Epoch: [3/10], Step: [201/313], Validation Acc: 86.56\n",
      "Epoch: [3/10], Step: [301/313], Validation Acc: 86.94\n",
      "Epoch: [4/10], Step: [101/313], Validation Acc: 85.7\n",
      "Epoch: [4/10], Step: [201/313], Validation Acc: 85.52\n",
      "Epoch: [4/10], Step: [301/313], Validation Acc: 86.16\n",
      "Epoch: [5/10], Step: [101/313], Validation Acc: 85.08\n",
      "Epoch: [5/10], Step: [201/313], Validation Acc: 85.82\n",
      "Epoch: [5/10], Step: [301/313], Validation Acc: 85.16\n",
      "Epoch: [6/10], Step: [101/313], Validation Acc: 84.7\n",
      "Epoch: [6/10], Step: [201/313], Validation Acc: 85.4\n",
      "Epoch: [6/10], Step: [301/313], Validation Acc: 84.7\n",
      "Epoch: [7/10], Step: [101/313], Validation Acc: 84.94\n",
      "Epoch: [7/10], Step: [201/313], Validation Acc: 84.88\n",
      "Epoch: [7/10], Step: [301/313], Validation Acc: 84.98\n",
      "Epoch: [8/10], Step: [101/313], Validation Acc: 85.16\n",
      "Epoch: [8/10], Step: [201/313], Validation Acc: 84.34\n",
      "Epoch: [8/10], Step: [301/313], Validation Acc: 84.08\n",
      "Epoch: [9/10], Step: [101/313], Validation Acc: 83.8\n",
      "Epoch: [9/10], Step: [201/313], Validation Acc: 84.14\n",
      "Epoch: [9/10], Step: [301/313], Validation Acc: 84.1\n",
      "Epoch: [10/10], Step: [101/313], Validation Acc: 84.4\n",
      "Epoch: [10/10], Step: [201/313], Validation Acc: 84.38\n",
      "Epoch: [10/10], Step: [301/313], Validation Acc: 83.82\n",
      "Val Acc 84.28\n",
      "current config: \n",
      "n-gram:  1  max vocab size:  10000  embedding size:  800\n",
      "Epoch: [1/10], Step: [101/313], Validation Acc: 84.72\n",
      "Epoch: [1/10], Step: [201/313], Validation Acc: 87.24\n",
      "Epoch: [1/10], Step: [301/313], Validation Acc: 85.08\n",
      "Epoch: [2/10], Step: [101/313], Validation Acc: 86.72\n",
      "Epoch: [2/10], Step: [201/313], Validation Acc: 86.08\n",
      "Epoch: [2/10], Step: [301/313], Validation Acc: 87.18\n",
      "Epoch: [3/10], Step: [101/313], Validation Acc: 85.76\n",
      "Epoch: [3/10], Step: [201/313], Validation Acc: 86.5\n",
      "Epoch: [3/10], Step: [301/313], Validation Acc: 85.96\n",
      "Epoch: [4/10], Step: [101/313], Validation Acc: 86.06\n",
      "Epoch: [4/10], Step: [201/313], Validation Acc: 85.46\n",
      "Epoch: [4/10], Step: [301/313], Validation Acc: 82.12\n",
      "Epoch: [5/10], Step: [101/313], Validation Acc: 85.3\n",
      "Epoch: [5/10], Step: [201/313], Validation Acc: 85.66\n",
      "Epoch: [5/10], Step: [301/313], Validation Acc: 84.88\n",
      "Epoch: [6/10], Step: [101/313], Validation Acc: 84.82\n",
      "Epoch: [6/10], Step: [201/313], Validation Acc: 85.0\n",
      "Epoch: [6/10], Step: [301/313], Validation Acc: 84.84\n",
      "Epoch: [7/10], Step: [101/313], Validation Acc: 84.64\n",
      "Epoch: [7/10], Step: [201/313], Validation Acc: 84.52\n",
      "Epoch: [7/10], Step: [301/313], Validation Acc: 83.7\n",
      "Epoch: [8/10], Step: [101/313], Validation Acc: 84.8\n",
      "Epoch: [8/10], Step: [201/313], Validation Acc: 84.94\n",
      "Epoch: [8/10], Step: [301/313], Validation Acc: 84.14\n",
      "Epoch: [9/10], Step: [101/313], Validation Acc: 84.3\n",
      "Epoch: [9/10], Step: [201/313], Validation Acc: 83.7\n",
      "Epoch: [9/10], Step: [301/313], Validation Acc: 84.34\n",
      "Epoch: [10/10], Step: [101/313], Validation Acc: 84.2\n",
      "Epoch: [10/10], Step: [201/313], Validation Acc: 83.8\n",
      "Epoch: [10/10], Step: [301/313], Validation Acc: 84.16\n",
      "Val Acc 83.56\n",
      "current config: \n",
      "n-gram:  1  max vocab size:  20000  embedding size:  100\n",
      "Epoch: [1/10], Step: [101/313], Validation Acc: 79.48\n",
      "Epoch: [1/10], Step: [201/313], Validation Acc: 86.08\n",
      "Epoch: [1/10], Step: [301/313], Validation Acc: 86.66\n",
      "Epoch: [2/10], Step: [101/313], Validation Acc: 88.34\n",
      "Epoch: [2/10], Step: [201/313], Validation Acc: 88.1\n",
      "Epoch: [2/10], Step: [301/313], Validation Acc: 88.64\n",
      "Epoch: [3/10], Step: [101/313], Validation Acc: 88.42\n",
      "Epoch: [3/10], Step: [201/313], Validation Acc: 88.24\n",
      "Epoch: [3/10], Step: [301/313], Validation Acc: 88.06\n",
      "Epoch: [4/10], Step: [101/313], Validation Acc: 88.06\n",
      "Epoch: [4/10], Step: [201/313], Validation Acc: 87.72\n",
      "Epoch: [4/10], Step: [301/313], Validation Acc: 87.6\n",
      "Epoch: [5/10], Step: [101/313], Validation Acc: 87.28\n",
      "Epoch: [5/10], Step: [201/313], Validation Acc: 87.48\n",
      "Epoch: [5/10], Step: [301/313], Validation Acc: 87.12\n",
      "Epoch: [6/10], Step: [101/313], Validation Acc: 86.98\n",
      "Epoch: [6/10], Step: [201/313], Validation Acc: 86.56\n",
      "Epoch: [6/10], Step: [301/313], Validation Acc: 86.88\n",
      "Epoch: [7/10], Step: [101/313], Validation Acc: 86.76\n",
      "Epoch: [7/10], Step: [201/313], Validation Acc: 87.04\n",
      "Epoch: [7/10], Step: [301/313], Validation Acc: 87.08\n",
      "Epoch: [8/10], Step: [101/313], Validation Acc: 86.84\n",
      "Epoch: [8/10], Step: [201/313], Validation Acc: 86.66\n",
      "Epoch: [8/10], Step: [301/313], Validation Acc: 86.6\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [9/10], Step: [101/313], Validation Acc: 86.94\n",
      "Epoch: [9/10], Step: [201/313], Validation Acc: 86.66\n",
      "Epoch: [9/10], Step: [301/313], Validation Acc: 86.46\n",
      "Epoch: [10/10], Step: [101/313], Validation Acc: 86.42\n",
      "Epoch: [10/10], Step: [201/313], Validation Acc: 86.52\n",
      "Epoch: [10/10], Step: [301/313], Validation Acc: 86.54\n",
      "Val Acc 86.16\n",
      "current config: \n",
      "n-gram:  1  max vocab size:  20000  embedding size:  200\n",
      "Epoch: [1/10], Step: [101/313], Validation Acc: 82.72\n",
      "Epoch: [1/10], Step: [201/313], Validation Acc: 83.96\n",
      "Epoch: [1/10], Step: [301/313], Validation Acc: 84.8\n",
      "Epoch: [2/10], Step: [101/313], Validation Acc: 87.78\n",
      "Epoch: [2/10], Step: [201/313], Validation Acc: 88.08\n",
      "Epoch: [2/10], Step: [301/313], Validation Acc: 88.46\n",
      "Epoch: [3/10], Step: [101/313], Validation Acc: 88.36\n",
      "Epoch: [3/10], Step: [201/313], Validation Acc: 88.1\n",
      "Epoch: [3/10], Step: [301/313], Validation Acc: 86.86\n",
      "Epoch: [4/10], Step: [101/313], Validation Acc: 87.94\n",
      "Epoch: [4/10], Step: [201/313], Validation Acc: 87.3\n",
      "Epoch: [4/10], Step: [301/313], Validation Acc: 86.56\n",
      "Epoch: [5/10], Step: [101/313], Validation Acc: 87.06\n",
      "Epoch: [5/10], Step: [201/313], Validation Acc: 87.28\n",
      "Epoch: [5/10], Step: [301/313], Validation Acc: 87.08\n",
      "Epoch: [6/10], Step: [101/313], Validation Acc: 87.08\n",
      "Epoch: [6/10], Step: [201/313], Validation Acc: 86.44\n",
      "Epoch: [6/10], Step: [301/313], Validation Acc: 86.1\n",
      "Epoch: [7/10], Step: [101/313], Validation Acc: 86.74\n",
      "Epoch: [7/10], Step: [201/313], Validation Acc: 86.3\n",
      "Epoch: [7/10], Step: [301/313], Validation Acc: 86.68\n",
      "Epoch: [8/10], Step: [101/313], Validation Acc: 86.8\n",
      "Epoch: [8/10], Step: [201/313], Validation Acc: 86.76\n",
      "Epoch: [8/10], Step: [301/313], Validation Acc: 86.12\n",
      "Epoch: [9/10], Step: [101/313], Validation Acc: 86.62\n",
      "Epoch: [9/10], Step: [201/313], Validation Acc: 86.46\n",
      "Epoch: [9/10], Step: [301/313], Validation Acc: 86.2\n",
      "Epoch: [10/10], Step: [101/313], Validation Acc: 86.26\n",
      "Epoch: [10/10], Step: [201/313], Validation Acc: 86.2\n",
      "Epoch: [10/10], Step: [301/313], Validation Acc: 86.12\n",
      "Val Acc 86.12\n",
      "current config: \n",
      "n-gram:  1  max vocab size:  20000  embedding size:  500\n",
      "Epoch: [1/10], Step: [101/313], Validation Acc: 80.02\n",
      "Epoch: [1/10], Step: [201/313], Validation Acc: 81.18\n",
      "Epoch: [1/10], Step: [301/313], Validation Acc: 87.9\n",
      "Epoch: [2/10], Step: [101/313], Validation Acc: 88.16\n",
      "Epoch: [2/10], Step: [201/313], Validation Acc: 87.66\n",
      "Epoch: [2/10], Step: [301/313], Validation Acc: 86.96\n",
      "Epoch: [3/10], Step: [101/313], Validation Acc: 87.52\n",
      "Epoch: [3/10], Step: [201/313], Validation Acc: 87.34\n",
      "Epoch: [3/10], Step: [301/313], Validation Acc: 87.64\n",
      "Epoch: [4/10], Step: [101/313], Validation Acc: 87.18\n",
      "Epoch: [4/10], Step: [201/313], Validation Acc: 86.82\n",
      "Epoch: [4/10], Step: [301/313], Validation Acc: 86.8\n",
      "Epoch: [5/10], Step: [101/313], Validation Acc: 87.24\n",
      "Epoch: [5/10], Step: [201/313], Validation Acc: 86.76\n",
      "Epoch: [5/10], Step: [301/313], Validation Acc: 85.9\n",
      "Epoch: [6/10], Step: [101/313], Validation Acc: 86.86\n",
      "Epoch: [6/10], Step: [201/313], Validation Acc: 86.3\n",
      "Epoch: [6/10], Step: [301/313], Validation Acc: 86.86\n",
      "Epoch: [7/10], Step: [101/313], Validation Acc: 84.34\n",
      "Epoch: [7/10], Step: [201/313], Validation Acc: 86.58\n",
      "Epoch: [7/10], Step: [301/313], Validation Acc: 86.04\n",
      "Epoch: [8/10], Step: [101/313], Validation Acc: 86.1\n",
      "Epoch: [8/10], Step: [201/313], Validation Acc: 86.16\n",
      "Epoch: [8/10], Step: [301/313], Validation Acc: 86.02\n",
      "Epoch: [9/10], Step: [101/313], Validation Acc: 86.2\n",
      "Epoch: [9/10], Step: [201/313], Validation Acc: 86.02\n",
      "Epoch: [9/10], Step: [301/313], Validation Acc: 85.94\n",
      "Epoch: [10/10], Step: [101/313], Validation Acc: 85.94\n",
      "Epoch: [10/10], Step: [201/313], Validation Acc: 86.12\n",
      "Epoch: [10/10], Step: [301/313], Validation Acc: 85.88\n",
      "Val Acc 85.72\n",
      "current config: \n",
      "n-gram:  1  max vocab size:  20000  embedding size:  800\n",
      "Epoch: [1/10], Step: [101/313], Validation Acc: 84.58\n",
      "Epoch: [1/10], Step: [201/313], Validation Acc: 85.28\n",
      "Epoch: [1/10], Step: [301/313], Validation Acc: 86.6\n",
      "Epoch: [2/10], Step: [101/313], Validation Acc: 87.48\n",
      "Epoch: [2/10], Step: [201/313], Validation Acc: 87.2\n",
      "Epoch: [2/10], Step: [301/313], Validation Acc: 87.48\n",
      "Epoch: [3/10], Step: [101/313], Validation Acc: 86.78\n",
      "Epoch: [3/10], Step: [201/313], Validation Acc: 85.58\n",
      "Epoch: [3/10], Step: [301/313], Validation Acc: 87.04\n",
      "Epoch: [4/10], Step: [101/313], Validation Acc: 86.96\n",
      "Epoch: [4/10], Step: [201/313], Validation Acc: 86.0\n",
      "Epoch: [4/10], Step: [301/313], Validation Acc: 86.06\n",
      "Epoch: [5/10], Step: [101/313], Validation Acc: 86.6\n",
      "Epoch: [5/10], Step: [201/313], Validation Acc: 85.38\n",
      "Epoch: [5/10], Step: [301/313], Validation Acc: 86.26\n",
      "Epoch: [6/10], Step: [101/313], Validation Acc: 86.22\n",
      "Epoch: [6/10], Step: [201/313], Validation Acc: 86.68\n",
      "Epoch: [6/10], Step: [301/313], Validation Acc: 85.9\n",
      "Epoch: [7/10], Step: [101/313], Validation Acc: 86.18\n",
      "Epoch: [7/10], Step: [201/313], Validation Acc: 86.28\n",
      "Epoch: [7/10], Step: [301/313], Validation Acc: 86.22\n",
      "Epoch: [8/10], Step: [101/313], Validation Acc: 86.14\n",
      "Epoch: [8/10], Step: [201/313], Validation Acc: 86.06\n",
      "Epoch: [8/10], Step: [301/313], Validation Acc: 85.66\n",
      "Epoch: [9/10], Step: [101/313], Validation Acc: 85.84\n",
      "Epoch: [9/10], Step: [201/313], Validation Acc: 86.3\n",
      "Epoch: [9/10], Step: [301/313], Validation Acc: 85.6\n",
      "Epoch: [10/10], Step: [101/313], Validation Acc: 85.86\n",
      "Epoch: [10/10], Step: [201/313], Validation Acc: 85.62\n",
      "Epoch: [10/10], Step: [301/313], Validation Acc: 85.54\n",
      "Val Acc 85.64\n",
      "current config: \n",
      "n-gram:  1  max vocab size:  50000  embedding size:  100\n",
      "Epoch: [1/10], Step: [101/313], Validation Acc: 78.76\n",
      "Epoch: [1/10], Step: [201/313], Validation Acc: 86.66\n",
      "Epoch: [1/10], Step: [301/313], Validation Acc: 88.0\n",
      "Epoch: [2/10], Step: [101/313], Validation Acc: 88.72\n",
      "Epoch: [2/10], Step: [201/313], Validation Acc: 88.56\n",
      "Epoch: [2/10], Step: [301/313], Validation Acc: 89.08\n",
      "Epoch: [3/10], Step: [101/313], Validation Acc: 88.68\n",
      "Epoch: [3/10], Step: [201/313], Validation Acc: 88.64\n",
      "Epoch: [3/10], Step: [301/313], Validation Acc: 88.2\n",
      "Epoch: [4/10], Step: [101/313], Validation Acc: 88.52\n",
      "Epoch: [4/10], Step: [201/313], Validation Acc: 88.36\n",
      "Epoch: [4/10], Step: [301/313], Validation Acc: 88.14\n",
      "Epoch: [5/10], Step: [101/313], Validation Acc: 86.38\n",
      "Epoch: [5/10], Step: [201/313], Validation Acc: 88.12\n",
      "Epoch: [5/10], Step: [301/313], Validation Acc: 88.08\n",
      "Epoch: [6/10], Step: [101/313], Validation Acc: 87.84\n",
      "Epoch: [6/10], Step: [201/313], Validation Acc: 87.98\n",
      "Epoch: [6/10], Step: [301/313], Validation Acc: 87.86\n",
      "Epoch: [7/10], Step: [101/313], Validation Acc: 87.76\n",
      "Epoch: [7/10], Step: [201/313], Validation Acc: 87.8\n",
      "Epoch: [7/10], Step: [301/313], Validation Acc: 87.62\n",
      "Epoch: [8/10], Step: [101/313], Validation Acc: 87.8\n",
      "Epoch: [8/10], Step: [201/313], Validation Acc: 87.46\n",
      "Epoch: [8/10], Step: [301/313], Validation Acc: 87.42\n",
      "Epoch: [9/10], Step: [101/313], Validation Acc: 87.46\n",
      "Epoch: [9/10], Step: [201/313], Validation Acc: 87.64\n",
      "Epoch: [9/10], Step: [301/313], Validation Acc: 87.66\n",
      "Epoch: [10/10], Step: [101/313], Validation Acc: 87.7\n",
      "Epoch: [10/10], Step: [201/313], Validation Acc: 87.5\n",
      "Epoch: [10/10], Step: [301/313], Validation Acc: 87.42\n",
      "Val Acc 87.24\n",
      "current config: \n",
      "n-gram:  1  max vocab size:  50000  embedding size:  200\n",
      "Epoch: [1/10], Step: [101/313], Validation Acc: 81.56\n",
      "Epoch: [1/10], Step: [201/313], Validation Acc: 87.52\n",
      "Epoch: [1/10], Step: [301/313], Validation Acc: 88.38\n",
      "Epoch: [2/10], Step: [101/313], Validation Acc: 88.94\n",
      "Epoch: [2/10], Step: [201/313], Validation Acc: 88.56\n",
      "Epoch: [2/10], Step: [301/313], Validation Acc: 88.9\n",
      "Epoch: [3/10], Step: [101/313], Validation Acc: 88.7\n",
      "Epoch: [3/10], Step: [201/313], Validation Acc: 88.78\n",
      "Epoch: [3/10], Step: [301/313], Validation Acc: 87.96\n",
      "Epoch: [4/10], Step: [101/313], Validation Acc: 87.74\n",
      "Epoch: [4/10], Step: [201/313], Validation Acc: 88.32\n",
      "Epoch: [4/10], Step: [301/313], Validation Acc: 86.72\n",
      "Epoch: [5/10], Step: [101/313], Validation Acc: 87.96\n",
      "Epoch: [5/10], Step: [201/313], Validation Acc: 88.22\n",
      "Epoch: [5/10], Step: [301/313], Validation Acc: 88.04\n",
      "Epoch: [6/10], Step: [101/313], Validation Acc: 87.92\n",
      "Epoch: [6/10], Step: [201/313], Validation Acc: 88.08\n",
      "Epoch: [6/10], Step: [301/313], Validation Acc: 87.9\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [7/10], Step: [101/313], Validation Acc: 87.3\n",
      "Epoch: [7/10], Step: [201/313], Validation Acc: 87.1\n"
     ]
    }
   ],
   "source": [
    "def tune_n_gram(n_size, vocab_size_list, emb_dim_list):  \n",
    "    # set initials\n",
    "    new_train = []\n",
    "    new_val = []\n",
    "    max_val_acc = -1\n",
    "    best_config = []\n",
    "    train_size = len(train_data_tokens)\n",
    "    val_size = len(val_data_tokens)\n",
    "    \n",
    "    # tune n-gram\n",
    "    print(\"Tuning model hyperparameters:\")\n",
    "    for n_i in range(1,n_size+1):\n",
    "        for v_s in vocab_size_list:\n",
    "            # build vocab\n",
    "            token2id, id2token = build_vocab(n_i, all_train_tokens,v_s)\n",
    "            if n_i == 1:\n",
    "                new_train = train_data_tokens.copy()\n",
    "                new_val = val_data_tokens.copy()\n",
    "            else:\n",
    "                for idx in range(train_size):\n",
    "                    new_train[idx] += train_token_list[n_i-2][idx]\n",
    "                for idx in range(val_size):\n",
    "                    new_val[idx] += val_token_list[n_i -2][idx]\n",
    "\n",
    "            #print(new_train[0][len(new_train[0])-1])\n",
    "            #print(new_val[0][len(new_val[0])-1])\n",
    "            # get indices\n",
    "            train_data_indices = token2index_dataset(new_train,token2id)\n",
    "            val_data_indices = token2index_dataset(new_val, token2id)\n",
    "\n",
    "            # prepare data with dataloader for model\n",
    "            train_dataset = NewsGroupDataset(train_data_indices, train_target)\n",
    "            train_loader = torch.utils.data.DataLoader(dataset=train_dataset, \n",
    "                                                   batch_size=BATCH_SIZE,\n",
    "                                                   collate_fn=newsgroup_collate_func,\n",
    "                                                   shuffle=True)\n",
    "\n",
    "            val_dataset = NewsGroupDataset(val_data_indices, val_target)\n",
    "            val_loader = torch.utils.data.DataLoader(dataset=val_dataset, \n",
    "                                                       batch_size=BATCH_SIZE,\n",
    "                                                       collate_fn=newsgroup_collate_func,\n",
    "                                                       shuffle=True)\n",
    "\n",
    "\n",
    "            # run\n",
    "            for emb_i in emb_dim_list:\n",
    "                print(\"current config: \")\n",
    "                print(\"n-gram: \", n_i, \" max vocab size: \", v_s, \" embedding size: \", emb_i)\n",
    "                val_acc_i = run_model(emb_i, lr_default, num_epochs_default, train_loader, val_loader,id2token)\n",
    "                print (\"Val Acc {}\".format(val_acc_i))\n",
    "                if val_acc_i > max_val_acc:\n",
    "                    max_val_acc = val_acc_i\n",
    "                    best_config = [n_i, v_s, emb_i]  \n",
    "    return best_config\n",
    "\n",
    "    \n",
    "#     # update train data and val according to best n-gram size\n",
    "#     for i in range(1,best_n+1):\n",
    "#         for idx in range(train_size):\n",
    "#             train_data_tokens[idx]+= train_token_list[i-2][idx]\n",
    "#         for idx in range(val_size):\n",
    "#             val_data_tokens[idx] += val_token_list[i-2][idx]\n",
    "            \n",
    "    \n",
    "    \n",
    "#     # tune vocabulary size\n",
    "#     best_vocab_size = -1\n",
    "#     max_val_acc = -1\n",
    "    \n",
    "#     print(\"Tuning max vocabulary size:\")\n",
    "#     for v_s in vocab_size_list:\n",
    "#         print(\"max vocab size: \", v_s)\n",
    "#         token2id, id2token = build_vocab(best_n, v_s)\n",
    "        \n",
    "#         # get indices\n",
    "#         train_data_indices = token2index_dataset(train_data_tokens, token2id)\n",
    "#         val_data_indices = token2index_dataset(val_data_tokens, token2id)\n",
    "    \n",
    "#         # prepare data with dataloader for model\n",
    "#         train_dataset = NewsGroupDataset(train_data_indices, train_target)\n",
    "#         train_loader = torch.utils.data.DataLoader(dataset=train_dataset, \n",
    "#                                                batch_size=BATCH_SIZE,\n",
    "#                                                collate_fn=newsgroup_collate_func,\n",
    "#                                                shuffle=True)\n",
    "        \n",
    "#         val_dataset = NewsGroupDataset(val_data_indices, val_target)\n",
    "#         val_loader = torch.utils.data.DataLoader(dataset=val_dataset, \n",
    "#                                                    batch_size=BATCH_SIZE,\n",
    "#                                                    collate_fn=newsgroup_collate_func,\n",
    "#                                                    shuffle=True)\n",
    "        \n",
    "        \n",
    "        \n",
    "#         val_acc_i = run_model(emb_dim_default, lr_default, num_epochs_default, train_loader, val_loader, id2token)\n",
    "#         print (\"Val Acc {}\".format(val_acc_i))\n",
    "#         if val_acc_i > max_val_acc:\n",
    "#             max_val_acc = val_acc_i\n",
    "#             best_vocab_size = v_s\n",
    "    \n",
    "#     # update best vocab size\n",
    "#     token2id, id2token = build_vocab(best_n, best_vocab_size)\n",
    "#     # get indices\n",
    "#     train_data_indices = token2index_dataset(train_data_tokens, token2id)\n",
    "#     val_data_indices = token2index_dataset(val_data_tokens, token2id)\n",
    "    \n",
    "#     # prepare data with dataloader for model\n",
    "#     train_dataset = NewsGroupDataset(train_data_indices, train_target)\n",
    "#     train_loader = torch.utils.data.DataLoader(dataset=train_dataset, \n",
    "#                                                batch_size=BATCH_SIZE,\n",
    "#                                                collate_fn=newsgroup_collate_func,\n",
    "#                                                shuffle=True)\n",
    "        \n",
    "#     val_dataset = NewsGroupDataset(val_data_indices, val_target)\n",
    "#     val_loader = torch.utils.data.DataLoader(dataset=val_dataset, \n",
    "#                                                    batch_size=BATCH_SIZE,\n",
    "#                                                    collate_fn=newsgroup_collate_func,\n",
    "#                                                    shuffle=True)\n",
    "        \n",
    "    \n",
    "#     # tune embedding dimension\n",
    "#     best_emb_dim = -1\n",
    "#     max_val_acc = -1\n",
    "    \n",
    "#     print(\"Tuning embedding dimension:\")\n",
    "#     for emb_i in emb_dim_list:\n",
    "#         print(\"embedding dim: \", emb_i)\n",
    "#         val_acc_i = run_model(emb_i, lr_default, num_epochs_default, train_loader, val_loader, id2token)\n",
    "#         print (\"Val Acc {}\".format(val_acc_i))\n",
    "#         if val_acc_i > max_val_acc:\n",
    "#             max_val_acc = val_acc_i\n",
    "#             best_emb_dim = emb_i\n",
    "    \n",
    "#     return [best_n, best_vocab_size, best_emb_dim]\n",
    "    \n",
    "\n",
    "best_model_params = tune_n_gram(4, [10000,20000,50000,80000], [100,200,500,800])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Exercise 1\n",
    "### Try training the model with larger embedding size and for larger number of epochs\n",
    "### Also plot the training curves of the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2:\n",
    "### Try downloading IMDB Large Movie Review Dataset that is used for Assignment 1 http://ai.stanford.edu/~amaas/data/sentiment/\n",
    "### and tokenize it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3:\n",
    "### If you have time, after tokenizing the dataset try training Bag-of-Words model on it and report your initial results\n",
    "### on validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20000"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_data_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:nlpclass]",
   "language": "python",
   "name": "conda-env-nlpclass-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
